{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wellbee_api_testing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDVUarkY_zte",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb7741f-ebab-4dc9-96d4-cf49b72077e0"
      },
      "source": [
        "!pip install openai"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.7/dist-packages (0.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.41.1)\n",
            "Requirement already satisfied: requests>=2.20; python_version >= \"3.0\" in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20; python_version >= \"3.0\"->openai) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20; python_version >= \"3.0\"->openai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20; python_version >= \"3.0\"->openai) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20; python_version >= \"3.0\"->openai) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzN_fEFd_2Qw"
      },
      "source": [
        "import os\n",
        "from nltk import tokenize\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import openai\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Bidirectional\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import csv\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt \n",
        "import json\n",
        "import pickle"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L73qIvax_3fY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9b9136-4b4a-417a-eb76-1ac26679ae17"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo05M_Tb_4kI"
      },
      "source": [
        "openai.api_key = \"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\"\n",
        "completion = openai.Completion()"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26H3IhWT_50R"
      },
      "source": [
        "def load_sentiment_data():\n",
        "    df = pd.read_csv(\"data.csv\")\n",
        "    return df\n",
        "\n",
        "def lower_case(text_list):\n",
        "    lower_phrases = []\n",
        "    for phrase in text_list:\n",
        "        lower_phrases.append(phrase.lower())\n",
        "\n",
        "    return lower_phrases\n",
        "\n",
        "def tokenize_data(text_list, num_words=1000, oov_token=\"<UNK>\", pad_type=\"post\", trunc_type=\"post\"):\n",
        "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
        "    tokenizer.fit_on_texts(text_list)\n",
        "    word_index = tokenizer.word_index\n",
        "    train_sequences = tokenizer.texts_to_sequences(text_list)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    maxlen = max([len(x) for x in train_sequences])\n",
        "\n",
        "    train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n",
        "\n",
        "\n",
        "\n",
        "    return tokenizer, train_sequences, word_index, maxlen, train_padded"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gzyn1U1_5vZ"
      },
      "source": [
        "# load our machine learning models\n",
        "sentiment_model = load_model(\"sentiment_analysis (1).h5\")\n",
        "emotion_model = load_model(\"emotion_analysis.h5\")"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9dB1TO3_8zB"
      },
      "source": [
        "def ask(question, chatlog=None):\n",
        "  if chatlog is None:\n",
        "    chatlog = chat_log_beginning\n",
        "  prompt = f\"{chatlog}Human: {question}\\nAI:\"\n",
        "  response = completion.create(prompt=prompt, engine=\"davinci\", stop=[\"\\nHuman\"], temperature=0.9, top_p=1, frequency_penalty=0, presence_penalty=0.6, best_of=1, max_tokens=150)\n",
        "  answer = response.choices[0].text.strip()\n",
        "  return answer"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmoDiUAlM9JG"
      },
      "source": [
        "with open('tokenizer.pickle', 'rb') as f:\n",
        "    loaded_tokenizer = pickle.load(f)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgr0aBla_98S"
      },
      "source": [
        "def sentiment_analysis(tokenizer, json_file):\n",
        "    with open(json_file, \"r\") as f:\n",
        "      request_json = json.load(f)\n",
        "    secret = request_json[\"secret\"]\n",
        "\n",
        "    if secret != \"CONGPilDnoMinEThonYAnkoLViTypOlmStOd\":\n",
        "        return {\"code\": 401, \"error\": \"Unauthorized\"}\n",
        "    \n",
        "    text = request_json[\"text\"]\n",
        "    if text == \"\":\n",
        "        return {\"code\": 401, \"error\": \"Missing text\"}\n",
        "\n",
        "    \n",
        "    # splitting our paragraph into sentences\n",
        "    text_sentences = tokenize.sent_tokenize(text)\n",
        "    print(text_sentences)\n",
        "    \n",
        "\n",
        "    sentiment_classes = [\"negative\", \"neutral\", \"positive\"]\n",
        "    # converting our text to tokens\n",
        "    sentiments = []\n",
        "    tokenized_sentences = tokenizer.texts_to_sequences([text])\n",
        "    print(tokenized_sentences)\n",
        "    sentiment_prediction = sentiment_model.predict(tokenized_sentences)\n",
        "    pred = sentiment_classes[np.argmax(sentiment_prediction)]\n",
        "    print(pred)\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct1tm9Fm__fA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d091cf6-811e-4693-b721-b4d461f1348a"
      },
      "source": [
        "prediction = sentiment_analysis(loaded_tokenizer, \"sentiment.json\")\n",
        "print(prediction)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['you are such a gem to sociey :)']\n",
            "[[129, 1, 17, 6, 1, 1, 1, 1]]\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 89) for input KerasTensor(type_spec=TensorSpec(shape=(None, 89), dtype=tf.float32, name='embedding_12_input'), name='embedding_12_input', description=\"created by layer 'embedding_12_input'\"), but it was called on an input with incompatible shape (None, 8).\n",
            "WARNING:tensorflow:5 out of the last 21 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f46d3244290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "positive\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjpxd7IbX4Rh"
      },
      "source": [
        "def emotion_analysis(tokenizer, json_file):\n",
        "    with open(json_file, \"r\") as f:\n",
        "      request_json = json.load(f)\n",
        "    secret = request_json[\"secret\"]\n",
        "\n",
        "    if secret != \"CONGPilDnoMinEThonYAnkoLViTypOlmStOd\":\n",
        "        return {\"code\": 401, \"error\": \"Unauthorized\"}\n",
        "    \n",
        "    text = request_json[\"text\"]\n",
        "    if text == \"\":\n",
        "        return {\"code\": 401, \"error\": \"Missing text\"}\n",
        "\n",
        "    \n",
        "    # splitting our paragraph into sentences\n",
        "    text_sentences = tokenize.sent_tokenize(text)\n",
        "    print(text_sentences)\n",
        "    \n",
        "\n",
        "    sentiment_classes = [\"negative\", \"neutral\", \"positive\"]\n",
        "    # converting our text to tokens\n",
        "    sentiments = []\n",
        "    tokenized_sentences = tokenizer.texts_to_sequences([text])\n",
        "    print(tokenized_sentences)\n",
        "    sentiment_prediction = emotion_model.predict(tokenized_sentences)\n",
        "    pred = sentiment_classes[np.argmax(sentiment_prediction)]\n",
        "    print(pred)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7Tc1wJ7bcJo"
      },
      "source": [
        "emotion_analysis()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
